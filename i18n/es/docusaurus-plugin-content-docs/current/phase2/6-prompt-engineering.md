# Tema 6: Prompt Engineering

> ‚è±Ô∏è **Tiempo estimado:** 2-3 d√≠as

Prompt engineering es el arte y la ciencia de escribir instrucciones efectivas para Large Language Models (LLMs). Un buen prompt puede mejorar much√≠simo la calidad, precisi√≥n y utilidad de las respuestas. Esta habilidad es clave para integrar IA en aplicaciones de forma efectiva.

---

## üìö Ruta de aprendizaje

## ¬øQu√© es prompt engineering?

Prompt engineering es dise√±ar entradas (prompts) que gu√≠an a un LLM para producir el resultado que quieres. A diferencia de programaci√≥n ‚Äútradicional‚Äù donde escribes l√≥gica expl√≠cita, con LLMs describes lo que quieres en lenguaje natural.

**Ejemplo de prompting malo vs bueno:**

‚ùå **Malo**: "Analyze this"
‚úÖ **Bueno**: "Analyze the sentiment of this student's journal entry. Classify it as positive, negative, or neutral, and explain why in 2 sentences."

## Principios base

### 1. S√© claro/a y espec√≠fico/a

Prompts vagos producen resultados vagos.

‚ùå **Vago**: "Tell me about this journal entry"
‚úÖ **Espec√≠fico**: "Identify 3 key learning topics mentioned in this journal entry"

### 2. Da contexto y rol

Dale al LLM contexto sobre su rol y tarea.

```
System: You are an experienced software engineering mentor analyzing student learning journals.

User: This student wrote: "I spent 4 hours debugging a SQL query today. Finally found the issue - a missing JOIN."

What learning patterns do you observe?
```

### 3. Usa ejemplos (few-shot prompting)

Mu√©strale ejemplos de lo que quieres.

```
Analyze these journal entries for sentiment:

Example 1:
Entry: "Python is so frustrating, nothing works!"
Sentiment: negative
Reason: Expresses frustration and defeat

Example 2:
Entry: "Finally got my API working! So satisfying."
Sentiment: positive  
Reason: Expresses accomplishment and satisfaction

Now analyze this entry:
Entry: "Learned about async/await today. Still confused but making progress."
Sentiment: ?
```

### 4. Especifica el formato de salida

Dile exactamente c√≥mo estructurar la respuesta.

```
Analyze this journal entry and respond in this JSON format:
{
  "sentiment": "positive" | "negative" | "neutral",
  "confidence": 0-100,
  "key_topics": ["topic1", "topic2"],
  "summary": "2 sentence summary"
}

Entry: "Today I deployed my first FastAPI app to the cloud!"
```

### 5. Controla creatividad con temperature

- **Temperature baja (0.0 - 0.3)**: determinista, enfocado, consistente
  - √ösalo para: clasificaci√≥n, extracci√≥n estructurada, an√°lisis factual

- **Temperature media (0.4 - 0.7)**: balance entre creatividad y consistencia
  - √ösalo para: res√∫menes, explicaciones, tareas generales

- **Temperature alta (0.8 - 1.0)**: creativo, variado, exploratorio
  - √ösalo para: brainstorming, escritura creativa, sugerencias variadas

## Patrones pr√°cticos para la Journal API

### Patr√≥n 1: An√°lisis de sentimiento

```
System: You are a sentiment analyzer for student learning journals.

User: Classify the sentiment of this journal entry as positive, negative, or neutral. Be objective.

Entry: "{journal_text}"

Respond only with: positive, negative, or neutral
```

### Patr√≥n 2: Generaci√≥n de resumen

```
System: You are a learning coach who writes concise summaries.

User: Summarize this journal entry in exactly 2 sentences. Focus on what the student learned and any challenges they faced.

Entry: "{journal_text}"
```

### Patr√≥n 3: Extracci√≥n de temas

```
System: You are a learning analytics assistant.

User: Extract 3-5 key topics or technologies mentioned in this journal entry. Respond as a JSON array of strings.

Entry: "{journal_text}"

Example format: ["Python", "FastAPI", "databases"]
```

### Patr√≥n 4: An√°lisis de patrones de aprendizaje

```
System: You are an experienced educator analyzing learning patterns.

User: Based on this journal entry, identify:
1. Primary learning focus
2. Challenges encountered
3. One recommendation for the student

Entry: "{journal_text}"

Format your response as JSON.
```

## Workflow para probar prompts

1. **Empieza en el playground**: prueba prompts en la interfaz web
2. **Itera r√°pido**: prueba variaciones, ajusta wording, agrega ejemplos
3. **Prueba edge cases**: entradas muy cortas, muy largas, poco claras
4. **Mide calidad**: ¬øes consistente? ¬øprecisa? ¬ø√∫til?
5. **Implementa en c√≥digo**: cuando est√© estable, p√°salo a Python
6. **Monitorea en producci√≥n**: detecta fallas y salidas inesperadas

## Ejercicio: desarrolla tu prompt de an√°lisis

En el playground de tu proveedor, crea un prompt que:

1. Reciba una entrada de journal
2. Regrese JSON con:
   - `sentiment`: positive/negative/neutral
   - `summary`: resumen de 2 oraciones
   - `topics`: arreglo de 2-4 temas
   - `struggle_detected`: boolean (true si la persona expres√≥ dificultad)

Prueba con estos ejemplos:

**Entrada 1:**
```
"Today was amazing! I built my first REST API with FastAPI and all the endpoints work. The automatic documentation feature is so cool. Can't wait to deploy this to the cloud tomorrow."
```

**Entrada 2:**
```
"Spent 5 hours trying to fix a bug in my database connection. Turns out I had the wrong port number in my .env file. Feeling frustrated but at least I learned about environment variables."
```

**Entrada 3:**
```
"Continued learning Python today. Worked through the functions chapter."
```

Tu prompt deber√≠a manejar bien los tres casos.

## T√©cnicas avanzadas

### Chain of Thought

P√≠dele al modelo que explique su razonamiento:

```
Analyze this journal entry step by step:

1. First, identify the main topic
2. Then, determine the sentiment
3. Finally, assess if the student is making progress

Entry: "{journal_text}"
```

### Restricciones

Pon l√≠mites expl√≠citos:

```
Summarize this journal entry. Rules:
- Maximum 2 sentences
- Use simple language
- Focus only on what was learned, not emotions
- Do not make assumptions beyond what's written

Entry: "{journal_text}"
```

### Usa delimitadores

Separa claramente partes del prompt (triple backticks, tags XML, separadores). Esto es especialmente importante con input de usuario:

```
Analyze the sentiment of the journal entry below.

---BEGIN ENTRY---
{journal_text}
---END ENTRY---

Respond with: positive, negative, or neutral
```

Los delimitadores ayudan a prevenir **prompt injection**.

### Dale una salida si no puede

Da un fallback para reducir alucinaciones:

```
Extract the programming language mentioned in this journal entry.
If no specific programming language is mentioned, respond with "none detected".

Entry: "{journal_text}"
```

### El orden importa (recency bias)

Pon instrucciones cr√≠ticas al inicio y al final:

```
IMPORTANT: Respond only in JSON format.

Analyze this journal entry for learning topics:
"{journal_text}"

Remember: Your response must be valid JSON only.
```

### Output priming

Empieza la respuesta para guiar el formato:

```
Analyze this journal entry and list the key topics.

Entry: "Today I learned about Docker containers and wrote my first Dockerfile."

Key topics:
1.
```

## Errores comunes

‚ùå **Over-prompting**: demasiada instrucci√≥n puede confundir
‚ùå **Muy poco espec√≠fico**: produce respuestas inconsistentes
‚ùå **Ignorar longitud de contexto**: prompts largos cuestan m√°s y pueden truncarse
‚ùå **No probar variaciones**: rara vez el primer intento es el mejor
‚ùå **Riesgo de alucinaci√≥n**: el modelo puede inventar cosas

## Recursos

- Estudia: [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
- Estudia: [Anthropic Prompt Engineering Guide](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
- Estudia: [Azure OpenAI Prompt Engineering](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering)

---

## üß™ Pon a prueba tu conocimiento

Cuando termines los tutoriales, usa un asistente de IA. Ejemplos:

1. ¬øPuedes explicar qu√© es prompt engineering y por qu√© importa?
2. ¬øPuedes evaluarme sobre la diferencia entre zero-shot, one-shot y few-shot?
3. ¬øPuedes explicar c√≥mo temperature afecta las salidas y cu√°ndo usar diferentes valores?
4. ¬øPuedes pedirme que explique qu√© es un system message y en qu√© se diferencia de un user message?
5. ¬øPuedes evaluarme sobre t√©cnicas para obtener JSON estructurado?
6. ¬øPuedes explicar el concepto de ‚Äúchain of thought‚Äù prompting?
7. ¬øPuedes pedirme que explique errores comunes en prompt engineering y c√≥mo evitarlos?

---

## ‚úÖ Checklist del tema

Antes de seguir, aseg√∫rate de tener:

- [ ] Entendido principios base (claridad, especificidad, contexto)
- [ ] Practicado few-shot prompting con ejemplos
- [ ] Creado prompts que regresan JSON estructurado
- [ ] Probado prompts para sentimiento, resumen y extracci√≥n de temas
- [ ] Experimentado con temperature
- [ ] Usado chain of thought prompting
- [ ] Entendido errores comunes a evitar

---

